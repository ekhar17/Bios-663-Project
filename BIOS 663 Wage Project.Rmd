---
title: "BIOS 663 Group Project - Wage Data"
author: "Wage Groupies"
date: "5/3/2021"
output: html_document
---

```{r,message=FALSE,warning=FALSE}
#applicable packages
library(tidyverse)
library(dplyr)
library(mosaic)
library(car)
#load data - Andrew
wages <- read.csv("~/Bios-663-Project/CPS_85_Wages.txt", sep="")
#load data - Others
#load data - Elena
#wages <- read.csv("CPS_85_Wages.txt", sep="")
```

### Presentation (15 minutes)
1. description of the data
2. description and justification of the analysis 
3. analysis results (including relevant descriptive statistics, tables, and graphs)
4. conclusions and discussions

### EDA

```{r}
#head and missing values
head(wages)
#no missing values in the dataset
a <- sum(complete.cases(wages))
b <- nrow(wages)
print(a)
print(b)
```

```{r,include=FALSE}
##counts of binary variables
#South
tally(wages$south)
#Sex
tally(wages$sex)
#Union
tally(wages$union)
#Race
tally(wages$race)
#Occupation
tally(wages$occupation)
#Sector
tally(wages$sector)
#Marriage
tally(wages$marriage)
```

South|Count
----|----
Lives in South|156
Lives Elsewhere|378

Sex|Count
----|----
Male|289
Female|245

Union|Count
----|----
Union Member|96
Not Union Member|438

Race|Count
----|----
White|440
Hispanic|27
Other|67

Occupation|Count
----|----
Management|55
Sales|38
Clerical|97
Service|83
Professional|105
Other|156

Sector|Count
----|----
Manufacturing|99
Construction|24
Other|411

Marital Status|Count
----|----
Married|350
Unmarried|184


```{r}
##eda histograms of all numeric covariates
#education
wages %>% ggplot(aes(x=education)) + 
  geom_histogram(binwidth=1) + 
  labs(x='Years of Education', y='Count', title = 'Count of Years of Education')
#experience
wages %>% ggplot(aes(x=experience)) + 
  geom_histogram(binwidth=5) + 
  labs(x='Years of Experience', y='Count', title = 'Count of Years of Experience')
#age
wages %>% ggplot(aes(x=age)) + 
  geom_histogram(binwidth=5) + 
  labs(x='Years of Age', y='Count', title = 'Count of Years of Education')
#wage
wages %>% ggplot(aes(x=wage)) + 
  geom_histogram(binwidth=1) + 
  labs(x='Wage', y='Count', title = 'Count of Wages')
```

Correlation plot:
```{r}
library("corrplot")
M <- cor(wages)
corrplot(M, type = "upper")
```

### Interesting Metrics (group)

```{r}
#proportion of wages by sex
a <- wages %>% filter(sex == 1) %>% sum(wages$wage)
b <- wages %>% filter(sex == 0) %>% sum(wages$wage)
c <- a+b
prop_men <- a/c
prop_women <- b/c
print(prop_men)
print(prop_women)
#calculate proportion of men and women in data
d <- count(wages$sex==1)/nrow(wages)
e <- count(wages$sex==0)/nrow(wages)
print(d)
print(e)
```

- Women make up 47% of wages, Men make up 53% of wages but Women make up 46% of observations and men make up 54% of observations.

```{r}
#average wage by sex
tapply(wages$wage, wages$sex, mean)
```

```{r}
#average wage by education level
tapply(wages$wage, wages$education, mean)
```

```{r}
#average wage by residence area
tapply(wages$wage, wages$south, mean)
```

```{r}
#average wage by experience level
tapply(wages$wage, wages$experience, mean)
```

```{r}
#average wage by union classification
tapply(wages$wage, wages$union, mean)
```

```{r}
#average wage by age
tapply(wages$wage, wages$age, mean)
```

```{r}
#average wage by race level
tapply(wages$wage, wages$race, mean)
```

```{r}
#average wage by occupation
tapply(wages$wage, wages$occupation, mean)
```

```{r}
#average wage by sector
tapply(wages$wage, wages$sector, mean)
```

```{r}
#average wage by marital status
tapply(wages$wage, wages$marriage, mean)
```
##########################################################################

### Determining whether to transform variables
```{r}
## Initial Model
model_inital <- lm(wage ~ . ,data = wages)

## Check assumptions
stud_res = rstudent(model_initial)
hist(stud_res, main = "Histogram of the Studentized Residuals - Initial Model", breaks = 20, 
     xlab = "Studentized Residuals")
largest_res = stud_res %>% abs() %>% sort(decreasing = TRUE) %>% head(15)
largest_res
```
One residual has a a studentized residual of 9.21. This could be a potential outlier.
```{r}
wages[171,]
```
This 21 year old with 1 year of experience is getting $44.5/hour, which is a potential outlier. We will keep him in the analysis for now and then reevaluate whether the outlier should be removed for the calculation of the final model. 

We can see that there are many studentized residuals with values > 2, so the model does not fit all of the values well.

```{r}
plot(model_inital$fitted.values, stud_res, ylab = "Studentized Residuals", 
     xlab = "Predicted Values", main = "Studentized Residuals vs Predicted Values Plot - Initial Model")
abline(h=0, lty = 2, col = "blue")
## Heteroskadisity, has fan v shape
ks.test(stud_res, pnorm, 0, 1)
## p-value less than 0.05
```
This means the normality assumption is violated and the homogenous variance assumption is also violated. We will use boxcox to determine how to transform the dependent variable (wage).

```{r}
library(MASS)
y = MASS::boxcox(model_inital, plotit = TRUE,  lambda = seq(-.1, .1, by = 0.1))

```
The log likelihood is maximized at -0.029292929, so we round down to 0 for interpretability. This means that we will use an ln(y) transformation, which makes sense as this is the variance stabilizing transformation for count data.

We will briefly check to see if the transformation was helpful
```{r}
t_total_reg = lm(log(wage) ~ ., data = wages)

stud_res1 = rstudent(t_total_reg)
hist(stud_res1, main = "Histogram of the Studentized Residuals", breaks = 20, 
     xlab = "Studentized Residuals")
## Looks normal, longish right tail
plot(t_total_reg$fitted.values, stud_res1, ylab = "Studentized Residuals", 
     xlab = "Predicted Values", main = "Plot of Studentized Residuals versus Predicted Values")
abline(h=0, lty = 2, col = "blue")
## Less heteroskadicity, no clear trend
ks.test(stud_res1, pnorm, 0, 1) 
## P-value above 0.05, so not reject null, so no transforamtion is needed.

```

We see now the residuals are normally distributed (Kolmogorov Smirnov p-value > 0.05) the residuals have much more homoskedasticity, so the log transformation improved the model. Now we will check if we should transform any of the three continuos covariates.


First all of the variables are scaled to be centered around 0.
```{r}
wages$age = wages$age - mean(wages$age)
wages$education = wages$education - mean(wages$education)
wages$experience = wages$experience - mean(wages$experience)

x = wages$age
x1 = x * log(x)
summary(lm(log(wage)~ x + x1, data =  wages))
## Not significant, so not transform ages

x = wages$education
x1 = x * log(x)
summary(lm(log(wage)~ x + x1, data =  wages))
## Not significant, so not transform education

x = wages$experience
x1 = x * log(x)
summary(lm(log(wage)~ x + x1, data =  wages))
## Not significant, so not transform experience
```

So apart from centering around 0, none of the continuous predictors  will be transformed. 

Now we look at variance inflation factors to examine colinearity.

```{r}
ols_vif_tol(lm(log(wage) ~ ., data = wages)
)

```
We can see that there is high variance inflation for education, experience and age. First we just remove experience since it has the highest VIF and see how that has changed.

```{r}
ols_vif_tol(lm(log(wage) ~ education + south +sex + union+ age+race +occupation +sector +marriage, data = wages)
)

```

All the VIF's are close to one, meaning there is not high collinearity. Thus our full model will be 
```{r}
full_model = lm(log(wage) ~ education + south +sex + union+ age+race +occupation +sector +marriage, data = wages)
summary(full_model)
```


```{r}
#step-wise backward model selection beginning with max model
step(full_model, direction='backward')
```

```{r}
#reduced model
model.2 <- lm(wage ~ education+south+sex+experience+union+race+sector,data = wages)
summary(model.2)
```


```{r}
# Numerical predictors:
# Age, experience, education
colnames(wages)

wages %>% 
  ggplot(.) +
  geom_point(aes(y = wage, x = age, color = factor(race))) +
  facet_grid(union~sex)
```
